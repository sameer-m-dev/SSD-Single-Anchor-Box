{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SSD Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "59d85de9a64b4a06b4e2367b6e03428f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9f59935d670840d6a9b2b89873431cc8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_95772581c78a445fa13c57af336246c8",
              "IPY_MODEL_480b96c89e44416f8cc42994c83460f4"
            ]
          }
        },
        "9f59935d670840d6a9b2b89873431cc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "95772581c78a445fa13c57af336246c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_95bbdae41d204307be6557da2a714c5e",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 112,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 112,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c67c9782a3064da29a00951e66aa1c96"
          }
        },
        "480b96c89e44416f8cc42994c83460f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_866445a651f44346822b743f660e6d93",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 112/112 [1:27:30&lt;00:00, 46.88s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_43e8b22d80f64182b44c3cd5aac8a944"
          }
        },
        "95bbdae41d204307be6557da2a714c5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c67c9782a3064da29a00951e66aa1c96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "866445a651f44346822b743f660e6d93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "43e8b22d80f64182b44c3cd5aac8a944": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_duVpk819qo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# installing required libraries\n",
        "# !pip install numpy==1.16 tensorflow-object-detection-api"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gw4WUPWx6ga",
        "colab_type": "code",
        "outputId": "251f3580-b110-42d2-a285-9eb3095dc381",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# using tf v1\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovvUej2pf5t0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing required libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "from collections import defaultdict\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "if tf.__version__ < '1.15':\n",
        "    print(tf.__version__)\n",
        "    raise ImportError('Please upgrade your tensorflow installation to v1.15.* or later!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjfQfz3Df5uC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is needed to display the images.\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geVAQ0KU15AX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unzipping model files\n",
        "! unzip model.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_tck2tvf5uG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# paths to main folders: with frozen graph, with classes labels, \n",
        "# with all shelves images and with data\n",
        "PATH_TO_MODEL = 'model/frozen_inference_graph.pb'\n",
        "PATH_TO_LABELS = 'pack_detector/data/pack.pbtxt'\n",
        "PATH_TO_IMAGES = 'data/images/ShelfImages/'\n",
        "PATH_TO_DATA = 'data/'\n",
        "NUM_CLASSES = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMJBdW72f5uK",
        "colab_type": "code",
        "outputId": "92503c58-a4df-408f-b8b1-d4d85aa4177d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# load photos dataframe to get all evaluation images names\n",
        "photos = pd.read_pickle(f'{PATH_TO_DATA}photos.pkl')\n",
        "photos = photos[~photos.is_train]\n",
        "photos = photos.reset_index().drop(\"index\", axis=1)\n",
        "photos.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file</th>\n",
              "      <th>shelf_id</th>\n",
              "      <th>planogram_id</th>\n",
              "      <th>is_train</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>C1_P01_N1_S2_2.JPG</td>\n",
              "      <td>C1_P01</td>\n",
              "      <td>N1_S2_2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>C3_P07_N4_S2_1.JPG</td>\n",
              "      <td>C3_P07</td>\n",
              "      <td>N4_S2_1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>C2_P03_N1_S4_1.JPG</td>\n",
              "      <td>C2_P03</td>\n",
              "      <td>N1_S4_1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>C2_P03_N4_S2_1.JPG</td>\n",
              "      <td>C2_P03</td>\n",
              "      <td>N4_S2_1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>C2_P08_N2_S3_2.JPG</td>\n",
              "      <td>C2_P08</td>\n",
              "      <td>N2_S3_2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 file shelf_id planogram_id  is_train\n",
              "0  C1_P01_N1_S2_2.JPG   C1_P01      N1_S2_2     False\n",
              "1  C3_P07_N4_S2_1.JPG   C3_P07      N4_S2_1     False\n",
              "2  C2_P03_N1_S4_1.JPG   C2_P03      N1_S4_1     False\n",
              "3  C2_P03_N4_S2_1.JPG   C2_P03      N4_S2_1     False\n",
              "4  C2_P08_N2_S3_2.JPG   C2_P08      N2_S3_2     False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-NSSjG63EBR",
        "colab_type": "code",
        "outputId": "a7002c14-4b2e-431e-88d4-5cc4df4e50a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(photos)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "112"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh_fqz36f5uQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load frozen graph\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_MODEL, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dM-ut0-f5uU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load categories (we have only 1 category pack)\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(\n",
        "    label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QavDL19if5uY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's write function that executes detection\n",
        "def run_inference_for_single_image(image, image_tensor, sess, tensor_dict):\n",
        "    # Run inference\n",
        "    expanded_dims = np.expand_dims(image, 0)\n",
        "    output_dict = sess.run(tensor_dict, feed_dict={image_tensor: expanded_dims})\n",
        "    # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "    output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
        "    output_dict['detection_classes'] = output_dict['detection_classes'][0].astype(np.uint8)\n",
        "    output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "    output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "    return output_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4xxrx8of5ud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# it is useful to be able to run inference not only on the whole image,\n",
        "# but also on its parts\n",
        "# cutoff - minimum detection scrore needed to take box\n",
        "def run_inference_for_image_part(image_tensor, sess, tensor_dict, \n",
        "                                 image, cutoff, ax0, ay0, ax1, ay1):\n",
        "    boxes = []\n",
        "    im = image[ay0:ay1, ax0:ax1]\n",
        "    h, w, c = im.shape\n",
        "    output_dict = run_inference_for_single_image(im, image_tensor, sess, tensor_dict)\n",
        "    for i in range(100):\n",
        "        if output_dict['detection_scores'][i] < cutoff:\n",
        "            break\n",
        "        y0, x0, y1, x1, score = *output_dict['detection_boxes'][i], \\\n",
        "                                output_dict['detection_scores'][i]\n",
        "        x0, y0, x1, y1, score = int(x0*w), int(y0*h), \\\n",
        "                                int(x1*w), int(y1*h), \\\n",
        "                                int(score * 100)\n",
        "        boxes.append((x0+ax0, y0+ay0, x1+ax0, y1+ay0, score))\n",
        "    return boxes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0BSOWXrf5ug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# additional helper function to work not with coordinates but with percentages\n",
        "def run_inference_for_image_part_pcnt(image_tensor, sess, tensor_dict, \n",
        "                                 image, cutoff, p_ax0, p_ay0, p_ax1, p_ay1):\n",
        "    h, w, c = image.shape\n",
        "    max_x, max_y = w-1, h-1\n",
        "    return run_inference_for_image_part(\n",
        "                                image_tensor, sess, tensor_dict, \n",
        "                                image, cutoff, \n",
        "                                int(p_ax0*max_x), int(p_ay0*max_y), \n",
        "                                int(p_ax1*max_x), int(p_ay1*max_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTSyi4tUf5uk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to display image with bounding boxes\n",
        "def display_image_with_boxes(image, boxes, p_x0=0, p_y0=0, p_x1=1, p_y1=1):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    for x0, y0, x1, y1, score in boxes:\n",
        "        image = cv2.rectangle(image, (x0, y0), (x1, y1), (0,255,0), 5)\n",
        "    if p_x0 != 0 or p_y0 !=0 or p_x1 != 1 or p_y1 != 1:\n",
        "        h, w, c = image.shape\n",
        "        max_x, max_y = w-1, h-1\n",
        "        image = cv2.rectangle(image, \n",
        "                              (int(p_x0*max_x), int(p_y0*max_y)), \n",
        "                              (int(p_x1*max_x), int(p_y1*max_y)), (0,0,255), 5)\n",
        "    plt.figure(figsize=(14, 14))\n",
        "    plt.imshow(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLHeDgxYf5un",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initializations function\n",
        "def initialize_graph():\n",
        "    ops = tf.get_default_graph().get_operations()\n",
        "    all_tensor_names = {output.name\n",
        "                        for op in ops\n",
        "                        for output in op.outputs}\n",
        "    tensor_dict = {}\n",
        "    for key in ['num_detections', 'detection_boxes',\n",
        "                'detection_scores', 'detection_classes',\n",
        "                'detection_masks']:\n",
        "        tensor_name = key + ':0'\n",
        "        if tensor_name in all_tensor_names:\n",
        "            tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
        "    image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "    return image_tensor, tensor_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM1jLDkpf5u8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for non-maximum suppression\n",
        "def non_max_suppression(boxes, overlapThresh):\n",
        "    if len(boxes) == 0:\n",
        "        return np.array([]).astype(\"int\")\n",
        "\n",
        "    if boxes.dtype.kind == \"i\":\n",
        "        boxes = boxes.astype(\"float\")\n",
        " \n",
        "    pick = []\n",
        "\n",
        "    x1 = boxes[:,0]\n",
        "    y1 = boxes[:,1]\n",
        "    x2 = boxes[:,2]\n",
        "    y2 = boxes[:,3]\n",
        "    sc = boxes[:,4]\n",
        " \n",
        "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
        "    idxs = np.argsort(sc)\n",
        " \n",
        "    while len(idxs) > 0:\n",
        "        last = len(idxs) - 1\n",
        "        i = idxs[last]\n",
        "        pick.append(i)\n",
        " \n",
        "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
        "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
        "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
        "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
        " \n",
        "        w = np.maximum(0, xx2 - xx1 + 1)\n",
        "        h = np.maximum(0, yy2 - yy1 + 1)\n",
        "\n",
        "        #todo fix overlap-contains...\n",
        "        overlap = (w * h) / area[idxs[:last]]\n",
        "         \n",
        "        idxs = np.delete(idxs, np.concatenate(([last],\n",
        "            np.where(overlap > overlapThresh)[0])))\n",
        "    \n",
        "    return boxes[pick].astype(\"int\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rSJzA4kf5vB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# main function to do a sliding windows inference with non-maximum suppression\n",
        "def do_sliding_window_inference_with_nm_suppression(file, cutoff, display_image=False):\n",
        "    with detection_graph.as_default():\n",
        "        with tf.Session() as sess:\n",
        "            image_tensor, tensor_dict = initialize_graph()\n",
        "            image = cv2.imread(f'{PATH_TO_IMAGES}{file}')\n",
        "            h, w, c = image.shape\n",
        "            boxes = run_inference_for_image_part_pcnt(\n",
        "                image_tensor, sess, tensor_dict, image, cutoff, 0, 0, 1, 1)\n",
        "            a = np.array(boxes)\n",
        "            mean_dx = int(np.mean(a[:,2]-a[:,0]))\n",
        "            mean_dy = int(np.mean(a[:,3]-a[:,1]))\n",
        "            step_x, step_y = mean_dx, mean_dy\n",
        "            window_size = 2*mean_dy\n",
        "            boxes = []\n",
        "            y0 = 0\n",
        "            while y0 < h-1:\n",
        "                x0 = 0\n",
        "                while x0 < w-1:\n",
        "                    x1, y1 = x0 + window_size, y0 + window_size\n",
        "                    boxes += run_inference_for_image_part(\n",
        "                        image_tensor, sess, tensor_dict, image, cutoff, \n",
        "                        x0, y0, x1, y1)\n",
        "                    x0 += step_y\n",
        "                y0 += step_x\n",
        "            boxes = non_max_suppression(np.array(boxes), 0.5)\n",
        "            if display_image is True:\n",
        "              display_image_with_boxes(image, boxes, display_image=False)\n",
        "            return len(boxes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpCmSSkjf5vE",
        "colab_type": "code",
        "outputId": "fa1c5dc1-c165-49a2-f844-ac0688ca2d58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "59d85de9a64b4a06b4e2367b6e03428f",
            "9f59935d670840d6a9b2b89873431cc8",
            "95772581c78a445fa13c57af336246c8",
            "480b96c89e44416f8cc42994c83460f4",
            "95bbdae41d204307be6557da2a714c5e",
            "c67c9782a3064da29a00951e66aa1c96",
            "866445a651f44346822b743f660e6d93",
            "43e8b22d80f64182b44c3cd5aac8a944"
          ]
        }
      },
      "source": [
        "# getting count for number of products in each image for the Eval set and storing it in a dictionary\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "image2products = {}\n",
        "for i in tqdm(range(len(photos))):\n",
        "  no_of_packs = do_sliding_window_inference_with_nm_suppression(photos['file'][i], 0.6, display_image=False)\n",
        "  image2products[photos['file'][i]] = no_of_packs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59d85de9a64b4a06b4e2367b6e03428f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjIBjz3sAY7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# storing dict as a JSON file\n",
        "import json\n",
        "\n",
        "with open('image2products.json', 'w') as fp:\n",
        "    json.dump(image2products, fp, indent=2)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}